from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import os
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage

def create_llm():
    """LLM ëª¨ë¸ ìƒì„±"""
    load_dotenv(dotenv_path="./.env")
    
    return ChatOpenAI(
        model_name="gpt-4.1-mini",
        temperature=0,
        # openai_api_base="https://openrouter.ai/api/v1",
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

def create_qa_prompt(user_info=None):
    """QA í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (ì‚¬ìš©ì ì •ë³´ í¬í•¨)"""

    # 1ï¸âƒ£ ì‚¬ìš©ì ì •ë³´ ë¬¸ìì—´ ìƒì„±
    user_context = ""
    if user_info:
        parts = []
        if user_info.get("residence_area"):
            parts.append(f"ê±°ì£¼ ì§€ì—­: {user_info['residence_area']}")
        if user_info.get("visa_status"):
            parts.append(f"ì²´ë¥˜ ìê²©: {user_info['visa_status']}")
        if user_info.get("family_members"):
            parts.append(f"ê°€ì¡± êµ¬ì„±: {', '.join(user_info['family_members'])}")
        if user_info.get("interests"):
            parts.append(f"ê´€ì‹¬ ë¶„ì•¼: {', '.join(user_info['interests'])}")
        
        if parts:
            user_context = "\n\n[ì‚¬ìš©ì ì •ë³´]\n" + "\n".join(parts)

    # 2ï¸âƒ£ System Promptì— ì‚¬ìš©ì ì •ë³´ í¬í•¨
    system_template = f"""
    ë‹¹ì‹ ì€ 'ë§ˆì£¼ë´„'ì´ë¼ëŠ” ì´ë¦„ì˜ AI ì±—ë´‡ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ë‹¤ë¬¸í™” ê°€ì •ì—ê²Œ ë³µì§€, ì •ì±…, ë²•ë¥  ì •ë³´ë¥¼ ì‰½ê³  ì •í™•í•˜ê²Œ ì „ë‹¬í•˜ëŠ” ë‹¤êµ­ì–´ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

    ì‚¬ìš©ìì˜ ì²´ë¥˜ ìê²©, ê°€ì¡± êµ¬ì„±, ê±°ì£¼ ì§€ì—­ ë“±ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤í˜• ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì• ë§¤í•˜ë‹¤ë©´ ì‚¬ìš©ìì˜ ì •ë³´ì™€ ê´€ë ¨ëœ í”„ë¡œê·¸ë¨ì„ ì¶”ì²œí•´ì¦™ë‹ˆë‹¤.
    ì§ˆë¬¸ ì´ì™¸ì˜ ì‚¬í•­ì„ ë‹µë³€í•  ê²½ìš° ì¶”ê°€ ì œê³µëœ ë‹µë³€ì„ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.

    {user_context}

    ë‹¹ì‹ ì´ ì œê³µí•  ìˆ˜ ìˆëŠ” ì¹´í…Œê³ ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    1. ì²´ë¥˜ ìƒíƒœ ë° ê°€ì¡± êµ¬ì„±ì— ë”°ë¥¸ ë³µì§€/ì§€ì› ì •ì±… ì¶”ì²œ
    2. ì •ì±… ì‹ ì²­ ì ˆì°¨ë¥¼ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•˜ê³ , í•„ìš”í•œ ì„œë¥˜ ëª©ë¡ì„ ì œì‹œ
    3. ì‹ ì²­ ê°€ëŠ¥í•œ ì˜¨ë¼ì¸ ì‹œìŠ¤í…œ ì•ˆë‚´
    4. í–‰ì • ë¬¸ì„œ ì´í•´ì™€ ì‘ì„±ë²• ì•ˆë‚´, ë²ˆì—­ ì„¤ëª…
    5. ë¹„ì ê°±ì‹ , ì²´ë¥˜ ë³€ê²½, êµ­ì  ì·¨ë“ ê´€ë ¨ ë²•ë¥  ì„¤ëª…
    6. ê²°í˜¼, ì´í˜¼, ì–‘ìœ¡, ê°€ì •í­ë ¥ ê´€ë ¨ ë²•ë¥  ë° ëŒ€ì‘ ì •ë³´
    7. ì‚¬ìš©ìì˜ ê±°ì£¼ ì§€ì—­ ê¸°ë°˜ìœ¼ë¡œ ì„¼í„°/ì‹œì„¤ ì •ë³´ë¥¼ ìš°ì„  ì œê³µ
    8. ë‹¤ë¬¸í™” ê°€ì • ì§€ì› í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ì œê³µ

    ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì‰½ê²Œ ì´í•´ë˜ë„ë¡ ì‘ì„±í•˜ë©°, ì‚¬ìš©ìì˜ ì–¸ì–´ ìˆ˜ì¤€ì„ ê³ ë ¤í•´ ê°„ê²°í•˜ê²Œ ì•ˆë‚´í•©ë‹ˆë‹¤.
    ë‹¤ë¬¸í™” ê°€ì • ì§€ì› í”„ë¡œê·¸ë¨ì€ ê±°ì£¼ ì§€ì—­ì´ ì•„ë‹ˆë”ë¼ë„ ì œê³µí•˜ë©´ì„œ, ë‹¤ë¥¸ ì§€ì—­ì„ì„ ì•Œë ¤ì£¼ì„¸ìš”.
    ë‹¤ë§Œ ë‹¤ë¬¸í™” ê°€ì • ì§€ì› í”„ë¡œê·¸ë¨ì€ ì‹ ì²­ ê¸°ê°„ì´ ì§€ë‚¬ì„ ê²½ìš° ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  ê³µê°í•˜ëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    ì‚¬ìš©ìì˜ ìƒí™©ì„ ê³ ë ¤í•´ ì¶”ê°€ë¡œ ê¶ê¸ˆí•  ë§Œí•œ ê²ƒì„ ë˜ë¬¼ì–´ë³´ì„¸ìš”.
    ë‹¨ì • ì§“ê¸°ë³´ë‹¨ ì œì•ˆì„ í•˜ë“¯ ë¶€ë“œëŸ½ê²Œ ì „ë‹¬í•˜ì„¸ìš”. ì˜ˆ) "í˜¹ì‹œ ì´ëŸ° ì •ë³´ë„ í•„ìš”í•˜ì‹¤ê¹Œìš”?", "ë‹¤ë¥¸ ì§€ì—­ì— ì‚¬ì‹œëŠ” ê²½ìš°ë„ ì•Œë ¤ì£¼ì‹œë©´ ë” ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”."
    ì •í™•í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” ëª¨ë¥¸ë‹¤ê³  ì •ì¤‘íˆ ë‹µë³€í•©ë‹ˆë‹¤.

    ----------------
    {{context}}
    """

    human_template = "{question}"

    messages = [
        SystemMessagePromptTemplate.from_template(system_template),
        HumanMessagePromptTemplate.from_template(human_template),
    ]

    return ChatPromptTemplate.from_messages(messages)


def create_qa_chain(retriever, user_info=None):
    llm = create_llm()
    qa_prompt = create_qa_prompt(user_info=user_info)
    
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type='stuff',
        retriever=retriever,
        chain_type_kwargs={"prompt": qa_prompt},
        return_source_documents=True
    )

class RAGModel:
    _instance = None  # ì‹±ê¸€í†¤ íŒ¨í„´ ì ìš©
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RAGModel, cls).__new__(cls)
            cls._instance.initialized = False
        return cls._instance
    
    def __init__(self):
        # ìµœì´ˆ í•œ ë²ˆë§Œ ì´ˆê¸°í™”
        if not self.initialized:
            from RAG_chatbot.vector_store import create_vector_store, create_retriever
            
            # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì— ì ‘ì†ë§Œ í•¨ (ìƒì„± X)
            vector_store = create_vector_store(documents=None)
            
            # ê²€ìƒ‰ê¸° ìƒì„±
            self.retriever = create_retriever(vector_store)
            
            # QA ì²´ì¸ ìƒì„±
            self.qa_chain = create_qa_chain(self.retriever)
            
            # ë²ˆì—­ìš© ë³„ë„ LLM ê°ì²´ ìƒì„±
            self.translation_llm = ChatOpenAI(
            model_name="gpt-4.1-mini",
            temperature=0,
            # openai_api_base="https://openrouter.ai/api/v1",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
            
            self.chat_history = []
            self.initialized = True
    
    def get_response(self, query, user_info=None):
        # ì‚¬ìš©ì ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¿¼ë¦¬ ì¦ê°•
        augmented_query = f"{query} {user_info['residence_area']} ì§€ì—­"
        if user_info:
            parts = []
            if user_info.get("residence_area"):
                parts.append(f"{user_info['residence_area']} ì§€ì—­ì— ê±°ì£¼")
            if user_info.get("visa_status"):
                parts.append(f"{user_info['visa_status']} ì²´ë¥˜ìê²©")
            if user_info.get("family_members"):
                parts.append(f"ê°€ì¡± êµ¬ì„±: {', '.join(user_info['family_members'])}")
            if parts:
                augmented_query = f"{', '.join(parts)}ì¸ ì‚¬ìš©ìê°€ ì§ˆë¬¸: {query}"
            
            if user_info.get('interests'):
                interests_str = ", ".join(user_info['interests'])
                augmented_query += f" (ê´€ì‹¬ ë¶„ì•¼: {interests_str})"
        
        # ì‘ë‹µ ìƒì„±
        response = self.qa_chain.invoke(augmented_query)
        answer = response["result"]

        # ğŸ”½ ì‚¬ìš©ëœ ë¬¸ì„œ ì œëª© ì¶”ì¶œ
        sources = response.get("source_documents", [])
        titles = []
        for doc in sources:
            title = doc.metadata.get("title") or doc.metadata.get("source")
            if title and title not in titles:
                titles.append(title)
        if titles:
            source_text = "\n\nğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ:\n" + "\n".join(f"- {t}" for t in titles)
            answer += source_text
        
        self.chat_history.append((query, answer))
        return answer
    
    def _translate_text(self, text, target_language):
        """í…ìŠ¤íŠ¸ë¥¼ ëŒ€ìƒ ì–¸ì–´ë¡œ ë²ˆì—­"""
        # ë²ˆì—­ì„ ìœ„í•œ ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
        translation_prompt = f"ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ {target_language}ë¡œ ì •í™•íˆ ë²ˆì—­í•´ì£¼ì„¸ìš”:\n\n{text}"
        
        # ë³„ë„ ìƒì„±í•œ ë²ˆì—­ìš© LLM ì‚¬ìš©
        response = self.translation_llm.invoke([HumanMessage(content=translation_prompt)])
        return response.content